{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb1741a",
   "metadata": {},
   "source": [
    "# NLP Assignment 1: Thatâ€™s What I LIKE (GPU/Colab Version) ðŸš€\n",
    "\n",
    "This notebook mirrors the complete assignment but is optimized for:\n",
    "1. **Google Colab** (using free T4 GPU).\n",
    "2. **Apple Silicon** (M1/M2/M3 using MPS).\n",
    "\n",
    "**Instructions:**\n",
    "- If on Colab: Go to `Runtime` -> `Change runtime type` -> Select `T4 GPU`.\n",
    "- Upload `wordsim353/` folder and `word-test.v1.txt` to the file area if running on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaef8ad",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"Setting up environment...\")\n",
    "\n",
    "# 1. Set Working Directory (Essential for Local Mac Usage)\n",
    "# If we see a 'git' folder, we are likely in the parent dir. Enter it.\n",
    "# We check if we are already INSIDE 'git' (common if launched from there) or inconsistent.\n",
    "# The goal: ensure we write to git/models, not root/models.\n",
    "if os.path.exists('git') and os.path.isdir('git'):\n",
    "    os.chdir('git')\n",
    "    print(f\"ðŸ“‚ Changed directory to: {os.getcwd()}\")\n",
    "else:\n",
    "    # On Colab, we are in /content usually.\n",
    "    print(f\"ðŸ“‚ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# 2. Install Dependencies (Quietly)\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"â˜ï¸ Google Colab detected. Installing packages...\")\n",
    "    !pip install -q gensim flask nltk torch numpy\n",
    "\n",
    "# 3. Create directories structure if missing\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('nltk_data', exist_ok=True)\n",
    "\n",
    "# 4. Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… CUDA GPU Detected (Colab/Linux)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"âœ… Apple MPS GPU Detected (Mac)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Training might be slow.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b30ef",
   "metadata": {},
   "source": [
    "## 1. Data Loader\n",
    "\n",
    "Handles loading the Reuters corpus, building vocabulary, and numericalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90307462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "\n",
    "nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
    "if not os.path.exists(nltk_data_dir):\n",
    "    os.makedirs(nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/reuters')\n",
    "except LookupError:\n",
    "    print(\"Downloading reuters corpus...\")\n",
    "    nltk.download('reuters', download_dir=nltk_data_dir, quiet=True)\n",
    "    nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)\n",
    "    nltk.download('punkt_tab', download_dir=nltk_data_dir, quiet=True)\n",
    "\n",
    "import zipfile\n",
    "reuters_zip_path = os.path.join(nltk_data_dir, 'corpora', 'reuters.zip')\n",
    "reuters_dir_path = os.path.join(nltk_data_dir, 'corpora', 'reuters')\n",
    "\n",
    "if os.path.exists(reuters_zip_path) and not os.path.exists(reuters_dir_path):\n",
    "    print(f\"Unzipping {reuters_zip_path}...\")\n",
    "    with zipfile.ZipFile(reuters_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(nltk_data_dir, 'corpora'))\n",
    "    print(\"Unzipping complete.\")\n",
    "\n",
    "\n",
    "MIN_FREQ = 5 # Minimum frequency for words to be included in vocab\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, min_freq=MIN_FREQ):\n",
    "        self.min_freq = min_freq\n",
    "        self.categories = None # Use full corpus\n",
    "        print(f\"Loading Reuters corpus (Full)...\")\n",
    "        \n",
    "        self.sentences = reuters.sents()\n",
    "        self.corpus = [[word.lower() for word in sent] for sent in self.sentences]\n",
    "        \n",
    "        print(f\"Corpus size: {len(self.corpus)} sentences\")\n",
    "        \n",
    "        self.build_vocab()\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        print(\"Building vocabulary...\")\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        self.all_words = flatten(self.corpus)\n",
    "        self.word_count = Counter(self.all_words)\n",
    "        \n",
    "        self.vocab = [w for w, c in self.word_count.items() if c >= self.min_freq]\n",
    "        self.vocab.append('<UNK>')\n",
    "        \n",
    "        self.word2index = {w: i for i, w in enumerate(self.vocab)}\n",
    "        self.index2word = {i: w for w, i in self.word2index.items()}\n",
    "        self.voc_size = len(self.vocab)\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.voc_size}\")\n",
    "        \n",
    "    def get_numericalized_corpus(self):\n",
    "        unk_idx = self.word2index['<UNK>']\n",
    "        numericalized_corpus = []\n",
    "        for sent in self.corpus:\n",
    "            sent_indices = [self.word2index.get(w, unk_idx) for w in sent]\n",
    "            numericalized_corpus.append(sent_indices)\n",
    "        return numericalized_corpus\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "        \n",
    "    def get_word2index(self):\n",
    "        return self.word2index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606fe7d",
   "metadata": {},
   "source": [
    "## 2. Model Definitions\n",
    "\n",
    "PyTorch classes for Skipgram, Negative Sampling, and GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center) \n",
    "        outside_embedding    = self.embedding_outside(outside) \n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs) \n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) \n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum)) \n",
    "        return loss\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        center_embed   = self.embedding_center(center) \n",
    "        outside_embed  = self.embedding_outside(outside) \n",
    "        negative_embed = self.embedding_outside(negative) \n",
    "        \n",
    "        uovc = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) \n",
    "        ukvc = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) \n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + torch.sum(self.logsigmoid(ukvc), 1).unsqueeze(1)\n",
    "        return -torch.mean(loss)\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.center_embedding(center) \n",
    "        outside_embeds = self.outside_embedding(outside) \n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        return torch.sum(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a774d",
   "metadata": {},
   "source": [
    "## 3. Training Word2Vec (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "loader = DataLoader() # min_freq=5\n",
    "corpus = loader.get_numericalized_corpus()\n",
    "voc_size = loader.voc_size\n",
    "word2index = loader.get_word2index()\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "print(f\"Vocab Size: {voc_size}\")\n",
    "\n",
    "def generate_skipgrams(corpus, window_size=2):\n",
    "    skipgrams = []\n",
    "    for doc in corpus:\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            center = doc[i]\n",
    "            outside = []\n",
    "            for w in range(1, window_size+1):\n",
    "                outside.append(doc[i-w])\n",
    "                outside.append(doc[i+w])\n",
    "            \n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "    return skipgrams\n",
    "\n",
    "\n",
    "def get_batch(corpus, batch_size, window_size=2):\n",
    "    \n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    \n",
    "    while len(inputs) < batch_size:\n",
    "        doc_idx = np.random.randint(0, len(corpus))\n",
    "        doc = corpus[doc_idx]\n",
    "        \n",
    "        if len(doc) < 2 * window_size + 1:\n",
    "            continue\n",
    "            \n",
    "        center_idx = np.random.randint(window_size, len(doc) - window_size)\n",
    "        center = doc[center_idx]\n",
    "        \n",
    "        offsets = list(range(-window_size, 0)) + list(range(1, window_size + 1))\n",
    "        offset = np.random.choice(offsets)\n",
    "        outside = doc[center_idx + offset]\n",
    "        \n",
    "        inputs.append([center])\n",
    "        labels.append([outside])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs) #(batch_size, voc_size, emb_size) \n",
    "        \n",
    "        all_vocabs_embedding = self.embedding_outside(all_vocabs)\n",
    "        outside_embedding = self.embedding_outside(outside)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        return loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f'Using device: {device}') # Use CPU for now as default\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f'Using device: {device}') # Use CPU for now as default\n",
    "batch_size = 512 # Increased for faster pure-python batching\n",
    "emb_size   = 2 # Instruction says \"Compare... models\". Usually embedding size is larger but notebook used 2. \n",
    "EMB_SIZE = 10 \n",
    "\n",
    "model      = Skipgram(voc_size, EMB_SIZE).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "all_vocabs = torch.LongTensor(list(range(voc_size))).unsqueeze(0).expand(batch_size, voc_size).to(device)\n",
    "\n",
    "num_epochs = 5 # Reduced epochs because each epoch is now FULL pass (much longer)\n",
    "print(f\"Training Skipgram (Window Size=2, Emb Size={EMB_SIZE}, Epochs={num_epochs})...\")\n",
    "\n",
    "num_tokens = sum([len(doc) for doc in corpus])\n",
    "print(f\"Total Tokens: {num_tokens}\")\n",
    "fields_per_token = 2 # window size 2 -> 2 pairs left, 2 pairs right? No, dynamic.\n",
    "approx_total_pairs = num_tokens * 2 * 2 \n",
    "steps_per_epoch = max(1, approx_total_pairs // batch_size)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(num_epochs):    \n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        input_batch, label_batch = get_batch(corpus, batch_size, window_size=2)\n",
    "        input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "        label_tensor = torch.LongTensor(label_batch).to(device)\n",
    "        \n",
    "        loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print(f\"  Step {step+1}/{steps_per_epoch} | Loss: {loss.item():.6f}\")\n",
    "\n",
    "    end = time.time()\n",
    "    avg_loss = total_loss / steps_per_epoch\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Avg Loss: {avg_loss:.6f} | Time: {end-start:.4f}s\")\n",
    "\n",
    "save_path = 'models/skipgram_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "import json\n",
    "metrics_path = 'models/metrics.json'\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "else:\n",
    "    metrics = {}\n",
    "\n",
    "metrics['Skipgram'] = {\n",
    "    'window_size': 2,\n",
    "    'training_loss': avg_loss,\n",
    "    'training_time': time.time() - total_start\n",
    "}\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(\"Metrics saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8a089",
   "metadata": {},
   "source": [
    "## 4. Training Word2Vec (Negative Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "loader = DataLoader()\n",
    "corpus = loader.get_numericalized_corpus()\n",
    "voc_size = loader.voc_size\n",
    "word2index = loader.get_word2index()\n",
    "\n",
    "print(\"Preparing Unigram Table...\")\n",
    "word_count = loader.word_count\n",
    "total_words = sum(word_count.values())\n",
    "unigram_table = []\n",
    "z = 0.001\n",
    "for w in loader.vocab:\n",
    "    if w == '<UNK>': continue\n",
    "    idx = word2index[w]\n",
    "    uw = word_count[w] / total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([idx] * uw_alpha)\n",
    "\n",
    "if '<UNK>' in word2index:\n",
    "    unk_idx = word2index['<UNK>']\n",
    "    unigram_table.extend([unk_idx] * 10)\n",
    "\n",
    "print(f\"Unigram Table Size: {len(unigram_table)}\")\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        target_index = targets[i].item()\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if neg == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(nsample)\n",
    "    return torch.LongTensor(neg_samples)\n",
    "\n",
    "def get_batch(corpus, batch_size, window_size=2):\n",
    "    inputs, labels = [], []\n",
    "    while len(inputs) < batch_size:\n",
    "        doc_idx = np.random.randint(0, len(corpus))\n",
    "        doc = corpus[doc_idx]\n",
    "        if len(doc) < 2 * window_size + 1:\n",
    "            continue\n",
    "        center_idx = np.random.randint(window_size, len(doc) - window_size)\n",
    "        center = doc[center_idx]\n",
    "        offsets = list(range(-window_size, 0)) + list(range(1, window_size + 1))\n",
    "        offset = np.random.choice(offsets)\n",
    "        outside = doc[center_idx + offset]\n",
    "        inputs.append([center])\n",
    "        labels.append([outside])\n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size) # Use outside embedding for negative samples\n",
    "        \n",
    "        uovc = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        \n",
    "        ukvc = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + torch.sum(self.logsigmoid(ukvc), 1).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f'Using device: {device}')\n",
    "batch_size = 512\n",
    "emb_size   = 10\n",
    "model      = SkipgramNeg(voc_size, emb_size).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "k = 5 # negative samples\n",
    "\n",
    "print(f\"Training SkipgramNEG (Window=2, Emb={emb_size}, Epochs={num_epochs}, k={k})...\")\n",
    "\n",
    "num_tokens = sum([len(doc) for doc in corpus])\n",
    "print(f\"Total Tokens: {num_tokens}\")\n",
    "approx_total_pairs = num_tokens * 2 * 2 \n",
    "steps_per_epoch = max(1, approx_total_pairs // batch_size)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        input_batch, label_batch = get_batch(corpus, batch_size, window_size=2)\n",
    "        input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "        label_tensor = torch.LongTensor(label_batch).to(device)\n",
    "        \n",
    "        neg_tensor = negative_sampling(label_tensor, unigram_table, k).to(device)\n",
    "        \n",
    "        loss = model(input_tensor, label_tensor, neg_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % 1000 == 0:\n",
    "             print(f\"  Step {step+1}/{steps_per_epoch} | Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    end = time.time()\n",
    "    avg_loss = total_loss / steps_per_epoch\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Avg Loss: {avg_loss:.6f} | Time: {end-start:.4f}s\")\n",
    "\n",
    "save_path = 'models/skipgram_neg_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "import json\n",
    "metrics_path = 'models/metrics.json'\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "else:\n",
    "    metrics = {}\n",
    "\n",
    "metrics['SkipgramNeg'] = {\n",
    "    'window_size': 2,\n",
    "    'training_loss': avg_loss,\n",
    "    'training_time': time.time() - total_start\n",
    "}\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(\"Metrics saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e71356",
   "metadata": {},
   "source": [
    "## 5. Training GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d12846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import combinations_with_replacement\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "loader = DataLoader() # min_freq=5\n",
    "corpus = loader.get_numericalized_corpus()\n",
    "voc_size = loader.voc_size\n",
    "vocab = loader.vocab\n",
    "word2index = loader.get_word2index()\n",
    "\n",
    "print(\"Building Co-occurrence Matrix...\")\n",
    "X_ik = {}\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "skip_grams = []\n",
    "for doc in corpus:\n",
    "    for i in range(1, len(doc)-1):\n",
    "        center = doc[i]\n",
    "        start = max(0, i - WINDOW_SIZE)\n",
    "        end = min(len(doc), i + WINDOW_SIZE + 1)\n",
    "        outside = [doc[j] for j in range(start, end) if j != i]\n",
    "        \n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))\n",
    "            \n",
    "X_ik_skipgrams = Counter(skip_grams)\n",
    "\n",
    "\n",
    "\n",
    "weighting_dic = {}\n",
    "X_ik = {}\n",
    "\n",
    "def weighting(x_ij):\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    if x_ij < x_max:\n",
    "        return (x_ij / x_max)**alpha\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(\"Calculating weights...\")\n",
    "for bigram, count in X_ik_skipgrams.items():\n",
    "    X_ik[bigram] = count\n",
    "    weighting_dic[bigram] = weighting(count)\n",
    "\n",
    "\n",
    "skip_grams_keys = list(X_ik.keys())\n",
    "print(f\"Number of non-zero co-occurrences: {len(skip_grams_keys)}\")\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.center_embedding(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.outside_embedding(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        return torch.sum(loss)\n",
    "\n",
    "def get_batch(batch_size, skip_grams_keys, X_ik, weighting_dic):\n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    \n",
    "    indices = np.random.choice(len(skip_grams_keys), batch_size, replace=False)\n",
    "    \n",
    "    for i in indices:\n",
    "        pair = skip_grams_keys[i]\n",
    "        center, outside = pair\n",
    "        random_inputs.append([center])\n",
    "        random_labels.append([outside])\n",
    "        \n",
    "        cooc = X_ik[pair]\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        random_weightings.append([weighting_dic[pair]])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f'Using device: {device}')\n",
    "batch_size = 512\n",
    "emb_size   = 10\n",
    "model      = Glove(voc_size, emb_size).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10 # GloVe converges faster, but let's do 10 full passes\n",
    "print(f\"Training GloVe (Window=2, Emb={emb_size}, Epochs={num_epochs})...\")\n",
    "print(f\"Non-zero pairs: {len(skip_grams_keys)}\")\n",
    "\n",
    "steps_per_epoch = max(1, len(skip_grams_keys) // batch_size)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    random.shuffle(skip_grams_keys)\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        input_batch, target_batch, cooc_batch, weighting_batch = get_batch(batch_size, skip_grams_keys, X_ik, weighting_dic)\n",
    "        \n",
    "        input_batch  = torch.LongTensor(input_batch).to(device)\n",
    "        target_batch = torch.LongTensor(target_batch).to(device)\n",
    "        cooc_batch   = torch.FloatTensor(cooc_batch).to(device)\n",
    "        weighting_batch = torch.FloatTensor(weighting_batch).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print(f\"  Step {step+1}/{steps_per_epoch} | Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    end = time.time()\n",
    "    avg_loss = total_loss / steps_per_epoch\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Avg Loss: {avg_loss:.6f} | Time: {end-start:.4f}s\")\n",
    "\n",
    "save_path = 'models/glove_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "import json\n",
    "metrics_path = 'models/metrics.json'\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "else:\n",
    "    metrics = {}\n",
    "\n",
    "metrics['GloVe'] = {\n",
    "    'window_size': 2,\n",
    "    'training_loss': avg_loss,\n",
    "    'training_time': time.time() - total_start\n",
    "}\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(\"Metrics saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc932a",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Comparison\n",
    "Loads trained models and compares them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d668770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "\n",
    "loader = DataLoader() # min_freq=5\n",
    "word2index = loader.get_word2index()\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "vocab = loader.vocab\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        pass # Not needed for inference\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    def forward(self, center, outside, negative):\n",
    "        pass \n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        pass\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(f'Using device: {device}')\n",
    "EMB_SIZE = 10\n",
    "VOC_SIZE = loader.voc_size\n",
    "\n",
    "print(\"Loading models...\")\n",
    "\n",
    "model_sg = Skipgram(VOC_SIZE, EMB_SIZE)\n",
    "try:\n",
    "    model_path = 'models/skipgram_model.pth'\n",
    "    model_sg.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    print(\"Skipgram loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Skipgram load failed: {e}\")\n",
    "\n",
    "model_neg = SkipgramNeg(VOC_SIZE, EMB_SIZE)\n",
    "try:\n",
    "    model_path = 'models/skipgram_neg_model.pth'\n",
    "    model_neg.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    print(\"SkipgramNEG loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"SkipgramNEG load failed: {e}\")\n",
    "\n",
    "model_glove = Glove(VOC_SIZE, EMB_SIZE)\n",
    "try:\n",
    "    model_path = 'models/glove_model.pth'\n",
    "    model_glove.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    print(\"GloVe loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"GloVe load failed: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Loading Gensim GloVe (glove-twitter-25)...\")\n",
    "    model_gensim = api.load(\"glove-twitter-25\")\n",
    "    print(\"Gensim GloVe loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Gensim GloVe failed: {e}\")\n",
    "    model_gensim = None\n",
    "\n",
    "def get_vector(model, word, model_type='scratch'):\n",
    "    if model_type == 'gensim':\n",
    "        if word in model:\n",
    "            return model[word]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    if word not in word2index:\n",
    "        return None\n",
    "    idx = torch.LongTensor([word2index[word]])\n",
    "    \n",
    "    if isinstance(model, Skipgram) or isinstance(model, SkipgramNeg):\n",
    "        v = model.embedding_center(idx)\n",
    "        vec = v # Usually we just use v_c. But averaging is also common. Notebook 1 used (c+o)/2.\n",
    "        u = model.embedding_outside(idx)\n",
    "        vec = (v + u) / 2\n",
    "        return vec.detach().numpy()[0]\n",
    "    \n",
    "    if isinstance(model, Glove):\n",
    "        v = model.center_embedding(idx)\n",
    "        u = model.outside_embedding(idx)\n",
    "        vec = (v + u) / 2\n",
    "        return vec.detach().numpy()[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def evaluate_correlation(model, model_type='scratch'):\n",
    "    pairs = []\n",
    "    try:\n",
    "        data_path = 'wordsim353/combined.csv'\n",
    "        with open(data_path, 'r') as f:\n",
    "            next(f) # skip header\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 3:\n",
    "                    pairs.append((parts[0].lower(), parts[1].lower(), float(parts[2])))\n",
    "    except FileNotFoundError:\n",
    "        print(\"wordsim353/combined.csv not found.\")\n",
    "        return 0.0\n",
    "\n",
    "    preds = []\n",
    "    humans = []\n",
    "    \n",
    "    for w1, w2, score in pairs:\n",
    "        v1 = get_vector(model, w1, model_type)\n",
    "        v2 = get_vector(model, w2, model_type)\n",
    "        \n",
    "        if v1 is not None and v2 is not None:\n",
    "            sim = cosine_similarity(v1, v2)\n",
    "            preds.append(sim)\n",
    "            humans.append(score)\n",
    "            \n",
    "    if not preds:\n",
    "        return 0.0\n",
    "        \n",
    "    corr, _ = scipy.stats.spearmanr(preds, humans)\n",
    "    return corr\n",
    "\n",
    "def evaluate_analogies(model, model_type='scratch'):\n",
    "    \n",
    "    sem_correct = 0\n",
    "    sem_total = 0\n",
    "    syn_correct = 0\n",
    "    syn_total = 0\n",
    "    \n",
    "    current_section = None\n",
    "    \n",
    "    try:\n",
    "        data_path = 'word-test.v1.txt'\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.startswith(':'):\n",
    "                    current_section = line.strip()\n",
    "                    continue\n",
    "                \n",
    "                if current_section == ': capital-common-countries':\n",
    "                    is_semantic = True\n",
    "                elif current_section == ': past-tense':\n",
    "                    is_semantic = False\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                parts = line.lower().split()\n",
    "                if len(parts) != 4: continue\n",
    "                \n",
    "                w_a, w_b, w_c, w_d = parts\n",
    "                \n",
    "                v_a = get_vector(model, w_a, model_type)\n",
    "                v_b = get_vector(model, w_b, model_type)\n",
    "                v_c = get_vector(model, w_c, model_type)\n",
    "                \n",
    "                if v_a is None or v_b is None or v_c is None:\n",
    "                    continue\n",
    "                \n",
    "                target = v_b - v_a + v_c\n",
    "                \n",
    "                \n",
    "                if model_type == 'gensim':\n",
    "                    try:\n",
    "                        res = model.most_similar(positive=[w_b, w_c], negative=[w_a], topn=1)\n",
    "                        pred_word = res[0][0]\n",
    "                    except:\n",
    "                        pred_word = \"\"\n",
    "                else:\n",
    "                    \n",
    "                    if not hasattr(model, 'embeddings_matrix'):\n",
    "                        if isinstance(model, Skipgram) or isinstance(model, SkipgramNeg):\n",
    "                            model.embeddings_matrix = (model.embedding_center.weight + model.embedding_outside.weight).detach().numpy() / 2\n",
    "                        elif isinstance(model, Glove):\n",
    "                            model.embeddings_matrix = (model.center_embedding.weight + model.outside_embedding.weight).detach().numpy() / 2\n",
    "                            \n",
    "                    target = target / np.linalg.norm(target)\n",
    "                    \n",
    "                    sims = np.dot(model.embeddings_matrix, target)\n",
    "                    \n",
    "                    if not hasattr(model, 'norm_embeddings_matrix'):\n",
    "                        norm = np.linalg.norm(model.embeddings_matrix, axis=1, keepdims=True)\n",
    "                        model.norm_embeddings_matrix = model.embeddings_matrix / (norm + 1e-9)\n",
    "                        \n",
    "                    sims = np.dot(model.norm_embeddings_matrix, target)\n",
    "                    best_idx = np.argmax(sims)\n",
    "                    pred_word = index2word[best_idx]\n",
    "                \n",
    "                if pred_word == w_d:\n",
    "                    if is_semantic: sem_correct += 1\n",
    "                    else: syn_correct += 1\n",
    "                \n",
    "                if is_semantic: sem_total += 1\n",
    "                else: syn_total += 1\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(\"word-test.v1.txt not found.\")\n",
    "        \n",
    "    sem_acc = sem_correct / sem_total if sem_total > 0 else 0.0\n",
    "    syn_acc = syn_correct / syn_total if syn_total > 0 else 0.0\n",
    "    \n",
    "    return sem_acc, syn_acc\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "\n",
    "import json\n",
    "metrics_path = 'models/metrics.json'\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        training_metrics = json.load(f)\n",
    "else:\n",
    "    training_metrics = {}\n",
    "\n",
    "models = {\n",
    "    'Skipgram': (model_sg, 'scratch'),\n",
    "    'SkipgramNeg': (model_neg, 'scratch'),\n",
    "    'GloVe': (model_glove, 'scratch'),\n",
    "    'Gensim': (model_gensim, 'gensim')\n",
    "}\n",
    "\n",
    "print(f\"{'Model':<15} {'Window':<8} {'Loss':<10} {'Time(s)':<10} {'Sem Acc':<10} {'Syn Acc':<10} {'Spearman':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, (m, m_type) in models.items():\n",
    "    if m is None:\n",
    "        print(f\"{name:<15} {'N/A':<8} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10} {'N/A':<10}\")\n",
    "        continue\n",
    "        \n",
    "    t_metrics = training_metrics.get(name, {})\n",
    "    window = t_metrics.get('window_size', 'N/A')\n",
    "    loss = t_metrics.get('training_loss', 'N/A')\n",
    "    time_taken = t_metrics.get('training_time', 'N/A')\n",
    "    \n",
    "    if m_type == 'gensim':\n",
    "        window = 'N/A' # Pre-trained\n",
    "        loss = 'N/A'\n",
    "        time_taken = 'N/A'\n",
    "\n",
    "    if isinstance(loss, float): loss = f\"{loss:.4f}\"\n",
    "    if isinstance(time_taken, float): time_taken = f\"{time_taken:.2f}\"\n",
    "    if isinstance(window, int): window = str(window)\n",
    "\n",
    "    corr = evaluate_correlation(m, m_type)\n",
    "    sem, syn = evaluate_analogies(m, m_type)\n",
    "    \n",
    "    print(f\"{name:<15} {window:<8} {loss:<10} {time_taken:<10} {sem:<10.4f} {syn:<10.4f} {corr:<10.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- WordSim353 Detailed Results (MSE/Y_true Analysis) ---\")\n",
    "def get_correlation_details(model, model_type='scratch'):\n",
    "    pairs = []\n",
    "    try:\n",
    "        data_path = 'wordsim353/combined.csv'\n",
    "        with open(data_path, 'r') as f:\n",
    "            next(f) # skip header\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 3:\n",
    "                    pairs.append((parts[0].lower(), parts[1].lower(), float(parts[2])))\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "    details = []\n",
    "    for w1, w2, score in pairs:\n",
    "        v1 = get_vector(model, w1, model_type)\n",
    "        v2 = get_vector(model, w2, model_type)\n",
    "        \n",
    "        if v1 is not None and v2 is not None:\n",
    "            sim = cosine_similarity(v1, v2)\n",
    "            details.append((w1, w2, score, sim))\n",
    "    return details\n",
    "\n",
    "for name, (m, m_type) in models.items():\n",
    "    if m is None: continue\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    details = get_correlation_details(m, m_type)\n",
    "    if not details:\n",
    "        print(\"No data found.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"{'Word 1':<15} {'Word 2':<15} {'Human (Y)':<10} {'Model (Pred)':<12} {'Sq.Err':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    mse_sum = 0\n",
    "    count = 0\n",
    "    for w1, w2, human, pred in details[:15]:\n",
    "        \n",
    "        \n",
    "        print(f\"{w1:<15} {w2:<15} {human:<10.2f} {pred:<12.4f} {'-':<10}\")\n",
    "    print(f\"... (showing first 15 of {len(details)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d47221",
   "metadata": {},
   "source": [
    "## 7. Web Application Demo\n",
    "\n",
    "(Requires local execution or ngrok tunneling on Colab. See `app` folder for details.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
